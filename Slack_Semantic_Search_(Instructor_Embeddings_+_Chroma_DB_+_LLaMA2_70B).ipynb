{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TollanBerhanu/Semantic-search-on-Slack/blob/main/Slack_Semantic_Search_(Instructor_Embeddings_%2B_Chroma_DB_%2B_LLaMA2_70B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Semantic Search on Exported Slack Data"
      ],
      "metadata": {
        "id": "I99shZMWoyOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains an implementation of semantic search on exported slack data\n",
        "\n",
        "*   *This notebook utilizes the following tools:*\n",
        "\n",
        ">\n",
        "\n",
        "    1.   'Pandas' - to load and extract relevant information from the exported data\n",
        "    2.   'Instructor embedding model' - to generate embeddings for each message\n",
        "    3.   'Chroma' - to store and query the vector embeddings along with some metadata\n",
        "    4.   'LLaMA2 70B model' - (via together.ai) to present the results in natural language\n"
      ],
      "metadata": {
        "id": "xwobrIb-oZjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**API** Keys"
      ],
      "metadata": {
        "id": "sciMCYuJHB8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"-------------------------------------------------\"\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"-------------------------------------------------\""
      ],
      "metadata": {
        "id": "12dbz_yAGrTx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing dependencies"
      ],
      "metadata": {
        "id": "Zx9_YuP5oJwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gitpython to clone a github repo containing the exported slack data\n",
        "!pip install gitpython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWwTMsTDhZ1",
        "outputId": "6f889180-231e-482c-9b93-4f1e9c97ee3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.34-py3-none-any.whl (188 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/188.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/188.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.34 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Langchain and Chroma\n",
        "!pip -q install langchain chromadb"
      ],
      "metadata": {
        "id": "11lUKMYtmQ7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f516e2-8359-4d56-87c3-db8ee987a558"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.3/418.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ngrok to host an api endpoint from colab\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "BKYJVueRmJkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275ad70d-218c-404b-f1ad-562bb1c5361c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=a5b2759b84f0637522bd285473be4ffb51c594d35f2c367e0f90070f6ef831a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install flask cors to enable cors for all domains.\n",
        "!pip install -U flask-cors"
      ],
      "metadata": {
        "id": "tV7SvQwG6Ilu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657824e5-69c7-41be-fe4b-e664b671760c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.3)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fetching the slack data"
      ],
      "metadata": {
        "id": "28aIsFkVoM_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path where the exported slack data is stored in local storage\n",
        "slack_data_path = \"/content/slackdata/\""
      ],
      "metadata": {
        "id": "FjhyBDOmoeAS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this if you haven't already cloned the GitHub repository"
      ],
      "metadata": {
        "id": "elPjfiLOnQfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import git\n",
        "\n",
        "repo_url = \"https://github.com/iCog-Labs-Dev/slack-export-data.git\"\n",
        "# slack_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset/slack-data/'\n",
        "\n",
        "git.Repo.clone_from(repo_url, slack_data_path)"
      ],
      "metadata": {
        "id": "mnp8_O_4DiuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b00ce79-5afc-429c-c68b-0a4340bdca4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<git.repo.base.Repo '/content/slackdata/.git'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_channels(path):\n",
        "  df = pd.read_json(path + 'channels.json')\n",
        "\n",
        "  channel_ids = [id for id in df['id']]\n",
        "  channel_names = [ name for name in df['name']]\n",
        "\n",
        "  return pd.DataFrame({ 'channel_id': channel_ids, 'channel_name': channel_names } )\n",
        "\n",
        "channels = get_all_channels(slack_data_path)\n",
        "channels"
      ],
      "metadata": {
        "id": "l6XPLPVrqEaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "f96b373a-8e36-4507-d084-96ffc81d8c4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    channel_id  channel_name\n",
              "0  C05D1SE01B7        random\n",
              "1  C05D77W3N76       general\n",
              "2  C05D7863DRA          test\n",
              "3   C05ABCDE01  gptgenerated"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a12780d-2160-42e4-83fb-1497f1fa453e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channel_id</th>\n",
              "      <th>channel_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C05D1SE01B7</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C05D77W3N76</td>\n",
              "      <td>general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C05D7863DRA</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C05ABCDE01</td>\n",
              "      <td>gptgenerated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a12780d-2160-42e4-83fb-1497f1fa453e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a12780d-2160-42e4-83fb-1497f1fa453e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a12780d-2160-42e4-83fb-1497f1fa453e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3070b532-eca4-4087-98ae-ab6e9092dcd8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3070b532-eca4-4087-98ae-ab6e9092dcd8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3070b532-eca4-4087-98ae-ab6e9092dcd8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "\n",
        "# Return the metadata of each message in the channel\n",
        "def extract_channel_metadata(path, channel_name):\n",
        "  try:\n",
        "    daily_json_files = glob.glob(path + channel_name +'/*.json')  # use glob to get all the json files in the folder\n",
        "  except:\n",
        "    print(f'The channel name {channel_name} is invalid!')\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "  if not daily_json_files:  # return if the channel doesn't exist (or hasn't been exported yet)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  metadata = pd.DataFrame(columns = ['message', 'channel', 'date', 'time', 'user_id', 'user_name'])\n",
        "\n",
        "  # loop over the list of json files (each json file includes every message in that channel for a single day)\n",
        "  for f in daily_json_files:\n",
        "    with open(f, 'r') as file:  # open the daily json file\n",
        "        data = file.read()  # Read the contents\n",
        "        today_data = json.loads(data) # Parse the JSON data\n",
        "\n",
        "    today_date = f.split(\"/\")[-1]  # 'f' is the full file path and file name\n",
        "    print('Extracting...', today_date) # the file name is the date\n",
        "\n",
        "    # iterate through all the messages of the day\n",
        "    for msg_data in today_data:\n",
        "      # Skip if its a \"channel_join\" type message or if the actual message content is empty\n",
        "      if ('subtype' in msg_data) or (msg_data['text'] == \"\") or (msg_data['type'] != 'message'):\n",
        "        continue\n",
        "        # TODO: filter out any links, stickers, and other junk\n",
        "        # TODO: replace @Member references with their real names\n",
        "\n",
        "      metadata.loc[len(metadata)] = {\n",
        "            'message': '(' + today_date.split(\".json\")[0] + ') ' + msg_data['user_profile']['first_name'] + ': ' + msg_data['text'],\n",
        "            'channel': channel_name,\n",
        "            'date': today_date.split(\".json\")[0], # omit the file extension '.json'\n",
        "            'time': msg_data['ts'],\n",
        "            'user_id': msg_data['user'],\n",
        "            'user_name': msg_data['user_profile']['real_name'] # We can use 'first_name' to get the first name and 'real_name' to get the full name of the user\n",
        "      }\n",
        "\n",
        "  return metadata\n",
        "\n",
        "# extract_channel_metadata(slack_data_path, 'test')"
      ],
      "metadata": {
        "id": "bXruD2kzE7Ld"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generating Embeddings"
      ],
      "metadata": {
        "id": "hCa_VB44FBlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import ast\n",
        "\n",
        "url = 'https://hackingfaces.onrender.com/embed'\n",
        "embedding_model_url = 'https://huggingface.co/spaces/tollan/instructor-xl'\n",
        "# embedding_model_url = 'https://huggingface.co/spaces/tollan/sentence-transformers-embedding'\n",
        "\n",
        "# Get the list of embeddings for all messages in a channel\n",
        "def embed_channel_messages(channel_messages):\n",
        "  msg_list = channel_messages.astype(str).tolist()\n",
        "  post_data = {\n",
        "            'link': embedding_model_url ,\n",
        "            # 'query': \"['hi','hello']\"\n",
        "            'query': str(msg_list)\n",
        "          }\n",
        "\n",
        "  embeddings = requests.post(url, data = post_data, headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"})\n",
        "\n",
        "  return ast.literal_eval(embeddings.text)\n",
        "\n",
        "# Get the corresponding embedding for the user's query\n",
        "def embed_query(query):\n",
        "  post_data = {\n",
        "            'link': embedding_model_url ,\n",
        "            # 'query': \"['hi','hello']\"\n",
        "            'query': str([query])\n",
        "          }\n",
        "\n",
        "  embeddings = requests.post(url, data = post_data, headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}, timeout=120)\n",
        "\n",
        "  return ast.literal_eval(embeddings.text)[0]"
      ],
      "metadata": {
        "id": "hCqYnwcXUe23"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Storing the embeddings in Chroma DB\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb"
      ],
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")"
      ],
      "metadata": {
        "id": "3m5iAQbhXfNK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a collection object from an existing collection, by name. If it doesn't exist, create one.\n",
        "collection = client.get_or_create_collection(\n",
        "      name= \"slack_collection\",\n",
        "      metadata= {\"hnsw:space\": \"cosine\"},\n",
        "    )"
      ],
      "metadata": {
        "id": "HASSWsxJXg1y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning: Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible :(\n",
        "# client.delete_collection(name=\"slack_collection\")"
      ],
      "metadata": {
        "id": "llBVxIWRYeoX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_channel_embeddings(channel_embeddings, channel_metadata):\n",
        "\n",
        "  # parse the channel metadata to json\n",
        "  parsed_channel_metadata = json.loads(channel_metadata.to_json(orient=\"records\"))\n",
        "\n",
        "  # create IDs for the embeddings\n",
        "  # ids = [ (channel_name + str(ch)) for ch in enumerate(channel_embeddings) ] ... [channelname_0 -> ... -> channelname_n]\n",
        "  ids = [ str(hash(metadata['message'])) for metadata in parsed_channel_metadata ]\n",
        "  try:\n",
        "  # upsert the embeddings along with their metadata, into a Chroma collection\n",
        "    collection.upsert(\n",
        "      ids = ids,\n",
        "      embeddings = channel_embeddings,\n",
        "      metadatas = parsed_channel_metadata,\n",
        "      # documents = channel_metadata['channel'].astype(str).tolist()\n",
        "    )\n",
        "  except chromadb.errors.DuplicateIDError as duplicate_err:\n",
        "    print(f'This one exists already: {duplicate_err}')\n",
        "\n",
        "  # print(collection.peek()) # returns a list of the first 10 items in the collection\n",
        "  print(f'Total items in collection: { collection.count() }') # returns the number of items in the collection"
      ],
      "metadata": {
        "id": "Baf3JQpTevBe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "step = 15\n",
        "\n",
        "# Upsert channel's data to the vector db\n",
        "def upsert_channels(channel_names=[]):\n",
        "  if (channel_names == []):\n",
        "    channel_names = channels['channel_name'].tolist()\n",
        "\n",
        "  for idx, ch_name in enumerate(channel_names):\n",
        "    print(f'Upserting channel { str(idx+1) } of { str(len(channel_names)) }: \"{ch_name}\" ... ')\n",
        "\n",
        "    channel_metadata = extract_channel_metadata(slack_data_path, ch_name)\n",
        "\n",
        "    if (channel_metadata.empty):\n",
        "      print('-> The channel is empty / doesn\\'t exist!')\n",
        "      continue\n",
        "\n",
        "    no_messages = len(channel_metadata)\n",
        "\n",
        "    for start in range(0, no_messages, step):\n",
        "\n",
        "      end = min(no_messages, start+step)\n",
        "      channel_metadata_batch = channel_metadata[start:end]\n",
        "\n",
        "      print(f'-> Embedding Batch { math.ceil(end/step) }/{ math.ceil(no_messages/step) } ...')\n",
        "      # print(str(channel_metadata_batch['message']))\n",
        "\n",
        "      channel_embeddings = embed_channel_messages(channel_metadata_batch['message'])\n",
        "\n",
        "      # upsert_channel_embeddings(channel_embeddings[start:end], channel_metadata_batch)\n",
        "      upsert_channel_embeddings(channel_embeddings, channel_metadata_batch)"
      ],
      "metadata": {
        "id": "xyvh1D1zSLTS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upsert_channels() # upsert all channels\n",
        "upsert_channels(['random', 'test', 'general'])  # general, random, gptgenerated"
      ],
      "metadata": {
        "id": "-T18gCFHTKWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b15e46-1e83-40ce-c955-b3fe098e2500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserting channel 1 of 3: \"random\" ... \n",
            "Extracting... 2023-06-19.json\n",
            "-> Embedding Batch 1/1 ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the persisted database from disk, and use it as normal.\n",
        "# vectordb = Chroma(persist_directory= '/content/chroma_db')"
      ],
      "metadata": {
        "id": "A-h1y_eAHmD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Querying the messages from Chroma"
      ],
      "metadata": {
        "id": "Zp4z2dBnplNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_chroma(query):\n",
        "  # Generate embeddings for the query\n",
        "  print('Embedding query ...')\n",
        "  embedded_query = embed_query(query)\n",
        "\n",
        "  query_response = collection.query(\n",
        "      query_embeddings = embedded_query,\n",
        "      n_results = 20,\n",
        "      # where = {\"metadata_field\": \"is_equal_to_this\"},\n",
        "      where = {\n",
        "          # \"channel\": {\"$eq\": \"general\"}\n",
        "          # \"user_id\": {\"$in\": [\"U05D1SQDNSH\", \"U05DHDPL4FK\", \"U05CQ93C3FZ\", \"U05D4M7RGQ3\"]}\n",
        "      }\n",
        "      # where_document={\"$contains\":\"search_string\"}\n",
        "  )\n",
        "\n",
        "  # documents = query_response['documents']\n",
        "  scores = query_response['distances'][0]\n",
        "  metadatas = query_response['metadatas'][0]\n",
        "\n",
        "  context = ''\n",
        "\n",
        "  for idx, metadata in enumerate(query_response['metadatas'][0]):\n",
        "    context += metadata['message'] + '\\n'\n",
        "    metadata['score'] = 1 - scores[idx]\n",
        "\n",
        "  return {'context': context, 'metadata': metadatas}\n",
        "\n",
        "# get_data_from_chroma(\"Why was it good work?\")\n",
        "get_data_from_chroma(\"What did Tollan say was good work?\")\n",
        "# get_data_from_chroma(\"What are some models that are comparable to GPT 3?\")"
      ],
      "metadata": {
        "id": "7h7hrBSYk40W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Getting the response from LLaMA 2 - 7B"
      ],
      "metadata": {
        "id": "siLXR-XT0JoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install huggingface_hub tiktoken\n",
        "!pip -q install --upgrade together"
      ],
      "metadata": {
        "id": "E0lALSEbYVbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together_api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "together.api_key = together_api_key\n",
        "\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()\n",
        "\n",
        "# print the first model's name\n",
        "print(models[3]['name']), print(models[52]['name'])\n",
        "# List all available models\n",
        "# for idx, model in enumerate(models):\n",
        "#     print(idx, model['name'])"
      ],
      "metadata": {
        "id": "2J4PBG2SYdDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the llama2 70B model\n",
        "together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
      ],
      "metadata": {
        "id": "QZ5aHi9FYhU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    # @root_validator()\n",
        "    # def validate_environment(cls, values: Dict) -> Dict:\n",
        "    #     \"\"\"Validate that the API key is set.\"\"\"\n",
        "    #     api_key = get_from_dict_or_env(\n",
        "    #         values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "    #     )\n",
        "    #     values[\"together_api_key\"] = api_key\n",
        "    #     return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "41q7h-Pyn4Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"\\n<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ],
      "metadata": {
        "id": "8Tp5EKVuoGDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "RBtZm1VGoMyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "instruction = \"\"\"\n",
        "### Chat Messages (Context): \\n\\n{context} \\n\\n\\n\n",
        "### Chat History: \\n\\n{chat_history} \\nHuman: {user_input} \\nAssistant:\\n\"\"\"\n",
        "system_prompt = \"\"\"\n",
        "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
        "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
        "  You must answer the question based on only chat messages you are given.\n",
        "  Don't answer anything outside the context you are provided and do not respond with anything from your general knowledge.\n",
        "  Try to mention the ones that you get the context from.\n",
        "  You may also look at the chat history to get additional context if necessary.\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "  \"\"\"\n",
        "  # You may also read the chat history to get additional context\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_input\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "mkHV0CbXqQlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: What's an LLM? \\n Bob: It's an abbreviation for Large Langage Model.\", user_input=\"Hi, my name is Sam\")"
      ],
      "metadata": {
        "id": "uJwKcvOD_el3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: What's an LLM? \\nBob: It's an abbreviation for Large Langage Model. \\nAlice: Oh, I see. Can you give me some examples of LLMs? \\nBob: Sure, there's GPT4, Liama and Palms\", user_input=\"What does LLM stand for?\")\n"
      ],
      "metadata": {
        "id": "jO1dykn099_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: What's an LLM? \\nBob: It's an abbreviation for Large Langage Model. \\nAlice: Oh, I see. Can you give me some examples of LLMs? \\nBob: Sure, there's GPT4, Liama, Palms, RoBerta, Shanon and Transfirmers\", user_input=\"Can you give me some more examples?\")"
      ],
      "metadata": {
        "id": "Ep7nEiZA_efr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: I'm Alice. \\nJohn: Nice to meet you Alice, My name is John.\", user_input=\"What's my name?\")\n"
      ],
      "metadata": {
        "id": "6H8sH-az_v04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################\n",
        "\n",
        "def semantic_search(query):\n",
        "  data = get_data_from_chroma(query)\n",
        "\n",
        "  context = data['context']\n",
        "  metadata = data['metadata']\n",
        "\n",
        "  response = llm_chain.predict(context=context, user_input=query)\n",
        "\n",
        "  return {'response': str(response), 'metadata': metadata}"
      ],
      "metadata": {
        "id": "x1Wl6es6ei35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# semantic_search(\"Hello, my name is John.\")\n",
        "# semantic_search(\"What did Tollan say about semantic search?\")\n",
        "# semantic_search(\"What are some models that are comparable to GPT 3?\")\n",
        "semantic_search(\"How can I make some pancakes?\")\n",
        "# semantic_search(\"What's my name?\")\n",
        "# semantic_search(\"Alright, Thanks!\")"
      ],
      "metadata": {
        "id": "m_-udqH5l0GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Creating an API Endpoint"
      ],
      "metadata": {
        "id": "pB-U-Yxjp4Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "port_no = 5000\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "ngrok_auth_token = os.environ[\"NGROK_AUTH_TOKEN\"]\n",
        "\n",
        "ngrok.set_auth_token( ngrok_auth_token )\n",
        "public_url =  ngrok.connect(port_no).public_url\n",
        "\n",
        "@app.route(\"/\", methods=['GET', 'POST'])\n",
        "def semantic_search_query():\n",
        "\n",
        "  if request.method == 'GET':\n",
        "    query = request.args.get('query')\n",
        "    return semantic_search(query)\n",
        "\n",
        "  elif request.method == 'POST':\n",
        "    query = request.json['query']\n",
        "    return semantic_search(query)\n",
        "\n",
        "\n",
        "print(f\"Public url for the API... {public_url}\")\n",
        "\n",
        "app.run(port=port_no)"
      ],
      "metadata": {
        "id": "2DeAXHDCkz5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmlOIW8yAPJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}