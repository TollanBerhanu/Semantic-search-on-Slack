{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TollanBerhanu/Semantic-search-on-Slack/blob/main/Slack_Semantic_Search_(NLP_Cloud_%2B_Chroma_DB_%2B_LLaMA2_70B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I99shZMWoyOV"
      },
      "source": [
        "# Implementing Semantic Search on Exported Slack Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwobrIb-oZjb"
      },
      "source": [
        "This notebook contains an implementation of semantic search on exported slack data\n",
        "\n",
        "*   *This notebook utilizes the following tools:*\n",
        "\n",
        ">\n",
        "\n",
        "    1.   'Pandas' - to load and extract relevant information from the exported data\n",
        "    2.   'Sentence Transformers embedding' - (via NLP Cloud) to generate embeddings for each message\n",
        "    3.   'Chroma' - to store and query the vector embeddings along with some metadata\n",
        "    4.   'LLaMA2 70B model' - (via together.ai) to present the results in natural language\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sciMCYuJHB8t"
      },
      "source": [
        "##**API** Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12dbz_yAGrTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b423b789-a36d-490d-97dd-2fc109df8713"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Together API key:  ··········\n",
            "Ngrok Auth Token:  ··········\n",
            "NLPCloud API Key:  ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = getpass.getpass('Together API key:  ')\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = getpass.getpass('Ngrok Auth Token:  ')\n",
        "os.environ[\"NLPCLOUD_API_KEY\"] = getpass.getpass('NLPCloud API Key:  ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9_YuP5oJwk"
      },
      "source": [
        "## 1. Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWwTMsTDhZ1",
        "outputId": "26b9969d-782d-48c9-cac2-1cdbaf5a3e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.34-py3-none-any.whl (188 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/188.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/188.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.6/188.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.34 smmap-5.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install gitpython to clone a github repo containing the exported slack data\n",
        "!pip install gitpython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11lUKMYtmQ7x",
        "outputId": "740d19c1-6ed9-42b2-bd16-98aa12112979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m418.3/418.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install Langchain and Chroma\n",
        "!pip -q install langchain chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKYJVueRmJkx",
        "outputId": "57609989-1cb1-4751-fdfd-1fdda7801fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=906041bde8b1bfbdd84dfb6bd4adaaa4b0e490bafd733ad1820369b5b04248df\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install ngrok to host an api endpoint from colab\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV7SvQwG6Ilu",
        "outputId": "0d48b8fe-b162-45ec-e9f6-41eb0bbb5314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.3)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-4.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install flask cors to enable cors for all domains.\n",
        "!pip install -U flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLP Cloud python client\n",
        "!pip install -U nlpcloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0br7TVYPVUV",
        "outputId": "2498b70c-2bd6-44fa-96da-e45a32c5f3d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpcloud in /usr/local/lib/python3.10/dist-packages (1.1.44)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from nlpcloud) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->nlpcloud) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->nlpcloud) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->nlpcloud) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->nlpcloud) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28aIsFkVoM_y"
      },
      "source": [
        "## 2. Fetching the slack data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjhyBDOmoeAS"
      },
      "outputs": [],
      "source": [
        "# The path where the exported slack data is stored in local storage\n",
        "slack_data_path = \"/content/slackdata/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elPjfiLOnQfQ"
      },
      "source": [
        "Run this if you haven't already cloned the GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnp8_O_4DiuV",
        "outputId": "b7d5846a-1dd6-4f52-b184-96a0b8eff759"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<git.repo.base.Repo '/content/slackdata/.git'>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import git\n",
        "\n",
        "repo_url = \"https://github.com/iCog-Labs-Dev/slack-export-data.git\"\n",
        "# slack_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset/slack-data/'\n",
        "\n",
        "git.Repo.clone_from(repo_url, slack_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "l6XPLPVrqEaV",
        "outputId": "f1d6f6f8-88c2-472f-ed83-71452753fadf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    channel_id  channel_name\n",
              "0  C05D1SE01B7        random\n",
              "1  C05D77W3N76       general\n",
              "2  C05D7863DRA          test\n",
              "3   C05ABCDE01  gptgenerated"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6f69035-5fdf-4c0e-8d6a-9de032964ae5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channel_id</th>\n",
              "      <th>channel_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C05D1SE01B7</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C05D77W3N76</td>\n",
              "      <td>general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C05D7863DRA</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C05ABCDE01</td>\n",
              "      <td>gptgenerated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6f69035-5fdf-4c0e-8d6a-9de032964ae5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b6f69035-5fdf-4c0e-8d6a-9de032964ae5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b6f69035-5fdf-4c0e-8d6a-9de032964ae5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e3faf52-7d73-4492-be35-0984de8356ba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e3faf52-7d73-4492-be35-0984de8356ba')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e3faf52-7d73-4492-be35-0984de8356ba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_channels(path):\n",
        "  df = pd.read_json(path + 'channels.json')\n",
        "\n",
        "  channel_ids = [id for id in df['id']]\n",
        "  channel_names = [ name for name in df['name']]\n",
        "\n",
        "  return pd.DataFrame({ 'channel_id': channel_ids, 'channel_name': channel_names } )\n",
        "\n",
        "channels = get_all_channels(slack_data_path)\n",
        "channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXruD2kzE7Ld"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import json\n",
        "\n",
        "# Return the metadata of each message in the channel\n",
        "def extract_channel_metadata(path, channel_name):\n",
        "  try:\n",
        "    daily_json_files = glob.glob(path + channel_name +'/*.json')  # use glob to get all the json files in the folder\n",
        "  except:\n",
        "    print(f'The channel name {channel_name} is invalid!')\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "  if not daily_json_files:  # return if the channel doesn't exist (or hasn't been exported yet)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  metadata = pd.DataFrame(columns = ['message', 'channel', 'date', 'time', 'user_id', 'user_name'])\n",
        "\n",
        "  # loop over the list of json files (each json file includes every message in that channel for a single day)\n",
        "  for f in daily_json_files:\n",
        "    with open(f, 'r') as file:  # open the daily json file\n",
        "        data = file.read()  # Read the contents\n",
        "        today_data = json.loads(data) # Parse the JSON data\n",
        "\n",
        "    today_date = f.split(\"/\")[-1]  # 'f' is the full file path and file name\n",
        "    print('Extracting...', today_date) # the file name is the date\n",
        "\n",
        "    # iterate through all the messages of the day\n",
        "    for msg_data in today_data:\n",
        "      # Skip if its a \"channel_join\" type message or if the actual message content is empty\n",
        "      if ('subtype' in msg_data) or (msg_data['text'] == \"\") or (msg_data['type'] != 'message'):\n",
        "        continue\n",
        "        # TODO: filter out any links, stickers, and other junk\n",
        "        # TODO: replace @Member references with their real names\n",
        "\n",
        "      metadata.loc[len(metadata)] = {\n",
        "            'message': '(' + today_date.split(\".json\")[0] + ') ' + msg_data['user_profile']['first_name'] + ': ' + msg_data['text'],\n",
        "            'channel': channel_name,\n",
        "            'date': today_date.split(\".json\")[0], # omit the file extension '.json'\n",
        "            'time': msg_data['ts'],\n",
        "            'user_id': msg_data['user'],\n",
        "            'user_name': msg_data['user_profile']['real_name'] # We can use 'first_name' to get the first name and 'real_name' to get the full name of the user\n",
        "      }\n",
        "\n",
        "  return metadata\n",
        "\n",
        "# extract_channel_metadata(slack_data_path, 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCa_VB44FBlC"
      },
      "source": [
        "## 3. Generating Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCqYnwcXUe23"
      },
      "outputs": [],
      "source": [
        "import nlpcloud\n",
        "\n",
        "url = 'https://hackingfaces.onrender.com/embed'\n",
        "embedding_model_url = 'https://huggingface.co/spaces/tollan/instructor-xl'\n",
        "# embedding_model_url = 'https://huggingface.co/spaces/tollan/sentence-transformers-embedding'\n",
        "\n",
        "# Get the list of embeddings for all messages in a channel\n",
        "def embed_channel_messages(channel_messages):\n",
        "  msg_list = channel_messages.astype(str).tolist()\n",
        "\n",
        "  client = nlpcloud.Client(\"paraphrase-multilingual-mpnet-base-v2\", \"8c4bde5fcd44994752f63535c1f545afa08f465c\")\n",
        "  # Returns json object.\n",
        "  embeddings = client.embeddings(msg_list)\n",
        "\n",
        "  return embeddings['embeddings']\n",
        "\n",
        "# Get the corresponding embedding for the user's query\n",
        "def embed_query(query):\n",
        "\n",
        "  client = nlpcloud.Client(\"paraphrase-multilingual-mpnet-base-v2\", \"8c4bde5fcd44994752f63535c1f545afa08f465c\")\n",
        "  # Returns json object.\n",
        "  embeddings = client.embeddings([query])\n",
        "\n",
        "  return embeddings['embeddings']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "## 4. Storing the embeddings in Chroma DB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "outputs": [],
      "source": [
        "import chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3m5iAQbhXfNK"
      },
      "outputs": [],
      "source": [
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HASSWsxJXg1y"
      },
      "outputs": [],
      "source": [
        "# Get a collection object from an existing collection, by name. If it doesn't exist, create one.\n",
        "collection = client.get_or_create_collection(\n",
        "      name= \"slack_collection\",\n",
        "      metadata= {\"hnsw:space\": \"cosine\"},\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llBVxIWRYeoX"
      },
      "outputs": [],
      "source": [
        "# Warning: Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible :(\n",
        "# client.delete_collection(name=\"slack_collection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Baf3JQpTevBe"
      },
      "outputs": [],
      "source": [
        "def upsert_channel_embeddings(channel_embeddings, channel_metadata):\n",
        "\n",
        "  # parse the channel metadata to json\n",
        "  parsed_channel_metadata = json.loads(channel_metadata.to_json(orient=\"records\"))\n",
        "\n",
        "  # create IDs for the embeddings\n",
        "  # ids = [ (channel_name + str(ch)) for ch in enumerate(channel_embeddings) ] ... [channelname_0 -> ... -> channelname_n]\n",
        "  ids = [ str(hash(metadata['message'])) for metadata in parsed_channel_metadata ]\n",
        "  try:\n",
        "  # upsert the embeddings along with their metadata, into a Chroma collection\n",
        "    collection.upsert(\n",
        "      ids = ids,\n",
        "      embeddings = channel_embeddings,\n",
        "      metadatas = parsed_channel_metadata,\n",
        "      # documents = channel_metadata['channel'].astype(str).tolist()\n",
        "    )\n",
        "  except chromadb.errors.DuplicateIDError as duplicate_err:\n",
        "    print(f'This one exists already: {duplicate_err}')\n",
        "\n",
        "  # print(collection.peek()) # returns a list of the first 10 items in the collection\n",
        "  print(f'Total items in collection: { collection.count() }') # returns the number of items in the collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyvh1D1zSLTS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "step = 50 # NLP Cloud's limit\n",
        "\n",
        "# Upsert channel's data to the vector db\n",
        "def upsert_channels(channel_names=[]):\n",
        "  if (channel_names == []):\n",
        "    channel_names = channels['channel_name'].tolist()\n",
        "\n",
        "  for idx, ch_name in enumerate(channel_names):\n",
        "    print(f'Upserting channel { str(idx+1) } of { str(len(channel_names)) }: \"{ch_name}\" ... ')\n",
        "\n",
        "    channel_metadata = extract_channel_metadata(slack_data_path, ch_name)\n",
        "\n",
        "    if (channel_metadata.empty):\n",
        "      print('-> The channel is empty / doesn\\'t exist!')\n",
        "      continue\n",
        "\n",
        "    no_messages = len(channel_metadata)\n",
        "\n",
        "    for start in range(0, no_messages, step):\n",
        "\n",
        "      end = min(no_messages, start+step)\n",
        "      channel_metadata_batch = channel_metadata[start:end]\n",
        "\n",
        "      print(f'-> Embedding Batch { math.ceil(end/step) }/{ math.ceil(no_messages/step) } ...')\n",
        "      # print(str(channel_metadata_batch['message']))\n",
        "\n",
        "      channel_embeddings = embed_channel_messages(channel_metadata_batch['message'])\n",
        "\n",
        "      # upsert_channel_embeddings(channel_embeddings[start:end], channel_metadata_batch)\n",
        "      upsert_channel_embeddings(channel_embeddings, channel_metadata_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T18gCFHTKWS",
        "outputId": "ce02730f-11ff-4b37-cdce-e2087565abe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserting channel 1 of 1: \"gptgenerated\" ... \n",
            "Extracting... 2023-07-01.json\n",
            "Extracting... 2023-06-19.json\n",
            "-> Embedding Batch 1/3 ...\n",
            "This one exists already: Expected IDs to be unique, found duplicates of: -8411118151397485211, 289350058731407828, 6902873516180614702\n",
            "Total items in collection: 31\n",
            "-> Embedding Batch 2/3 ...\n",
            "Total items in collection: 81\n",
            "-> Embedding Batch 3/3 ...\n",
            "Total items in collection: 107\n"
          ]
        }
      ],
      "source": [
        "# upsert_channels() # upsert all channels\n",
        "# upsert_channels(['random', 'test', 'general'])  # general, random, gptgenerated\n",
        "upsert_channels(['gptgenerated'])  # general, random, gptgenerated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-h1y_eAHmD-"
      },
      "outputs": [],
      "source": [
        "# Load the persisted database from disk, and use it as normal.\n",
        "# vectordb = Chroma(persist_directory= '/content/chroma_db')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp4z2dBnplNM"
      },
      "source": [
        "## 5. Querying the messages from Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h7hrBSYk40W",
        "outputId": "466311ab-bd96-4f40-a143-33e2b038782d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding query ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"(2023-06-19) Tollan: Also, welcome <@U05D4M7RGQ3>\\n(2023-06-19) Tollan: And we should also post some stickers... :grinning: :smile: :grin:\\n(2023-06-19) Tollan: Good work.. now we don't have to worry about exporting data.\\n(2023-06-19) Tollan: Oh.. u r right.. the search probably won't work properly with a small amount of data.\\n(2023-06-19) Tollan: It's best if we just post random topics here to test the semantic search.\\n(2023-06-19) Eyob: This is interesting\\n(2023-06-19) kenenisaalemayhu0: <https://haystack.deepset.ai/tutorials/08_preprocessing>\\n(2023-06-19) Eyob: Just for test case we should add some picture to see how slack handles it\\n(2023-06-19) kenenisaalemayhu0: yeah then we’ll see how we can clean the data\\n(2023-06-19) kenenisaalemayhu0: hello\\n(2023-06-19) Eyob: I didn't know slack had a dark mode until I saw the screenshot\\n(2023-06-19) Eyob: This is a good book for design nerds like me\\n(2023-06-19) kenenisaalemayhu0: Random shiii\\n(2023-06-19) Eyob: Bzw I already tries this a while ago but forgot to share this is what the export looks like\\n(2023-06-19) kenenisaalemayhu0: but we can’t use it for search since it’ll be very small\\n(2023-06-19) Eyob: Sample audio as well\\n\",\n",
              " 'metadata': [{'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: Also, welcome <@U05D4M7RGQ3>',\n",
              "   'time': '1687169174.199539',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.582626890628554},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: And we should also post some stickers... :grinning: :smile: :grin:',\n",
              "   'time': '1687169124.658349',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.5248079011782582},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Good work.. now we don't have to worry about exporting data.\",\n",
              "   'time': '1687166814.786429',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.4824761611539513},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Oh.. u r right.. the search probably won't work properly with a small amount of data.\",\n",
              "   'time': '1687168952.621439',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.40955398253291786},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: It's best if we just post random topics here to test the semantic search.\",\n",
              "   'time': '1687166901.338569',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.3196632531934416},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is interesting',\n",
              "   'time': '1687177484.530639',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.29883827778798344},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: <https://haystack.deepset.ai/tutorials/08_preprocessing>',\n",
              "   'time': '1687166202.864639',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.27121557397052465},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Just for test case we should add some picture to see how slack handles it',\n",
              "   'time': '1687177638.199629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.26993363193822983},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: yeah then we’ll see how we can clean the data',\n",
              "   'time': '1687167171.439409',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.24053318411125857},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: hello',\n",
              "   'time': '1687166197.580079',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.222195314747635},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Eyob: I didn't know slack had a dark mode until I saw the screenshot\",\n",
              "   'time': '1687177520.385329',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.21691262825468272},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is a good book for design nerds like me',\n",
              "   'time': '1687177700.175079',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.20606171100027104},\n",
              "  {'channel': 'random',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: Random shiii',\n",
              "   'time': '1687166237.151359',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.18311472723435696},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Bzw I already tries this a while ago but forgot to share this is what the export looks like',\n",
              "   'time': '1687188005.017139',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.1278853014382597},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: but we can’t use it for search since it’ll be very small',\n",
              "   'time': '1687167197.595189',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.09840637130038532},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Sample audio as well',\n",
              "   'time': '1687178138.528629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.09247496580438452}]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "def get_data_from_chroma(query):\n",
        "  # Generate embeddings for the query\n",
        "  print('Embedding query ...')\n",
        "  embedded_query = embed_query(query)\n",
        "\n",
        "  query_response = collection.query(\n",
        "      query_embeddings = embedded_query,\n",
        "      n_results = 20,\n",
        "      # where = {\"metadata_field\": \"is_equal_to_this\"},\n",
        "      where = {\n",
        "          # \"channel\": {\"$eq\": \"general\"}\n",
        "          # \"user_id\": {\"$in\": [\"U05D1SQDNSH\", \"U05DHDPL4FK\", \"U05CQ93C3FZ\", \"U05D4M7RGQ3\"]}\n",
        "      }\n",
        "      # where_document={\"$contains\":\"search_string\"}\n",
        "  )\n",
        "\n",
        "  # documents = query_response['documents']\n",
        "  scores = query_response['distances'][0]\n",
        "  metadatas = query_response['metadatas'][0]\n",
        "\n",
        "  context = ''\n",
        "\n",
        "  for idx, metadata in enumerate(query_response['metadatas'][0]):\n",
        "    context += metadata['message'] + '\\n'\n",
        "    metadata['score'] = 1 - scores[idx]\n",
        "\n",
        "  return {'context': context, 'metadata': metadatas}\n",
        "\n",
        "# get_data_from_chroma(\"Why was it good work?\")\n",
        "get_data_from_chroma(\"What did Tollan say was good work?\")\n",
        "# get_data_from_chroma(\"What are some models that are comparable to GPT 3?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLXR-XT0JoI"
      },
      "source": [
        "## 6. Getting the response from LLaMA 2 - 7B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0lALSEbYVbi",
        "outputId": "053827ce-787f-4151-ccbc-62d707071dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip -q install huggingface_hub tiktoken\n",
        "!pip -q install --upgrade together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J4PBG2SYdDr",
        "outputId": "e9d17da3-7560-4f47-a98f-48410525f502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EleutherAI/pythia-2.8b-v0\n",
            "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together_api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "together.api_key = together_api_key\n",
        "\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()\n",
        "\n",
        "# print the first model's name\n",
        "print(models[3]['name']), print(models[52]['name'])\n",
        "# List all available models\n",
        "# for idx, model in enumerate(models):\n",
        "#     print(idx, model['name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ5aHi9FYhU7",
        "outputId": "613244da-b1f0-4044-a1f0-1866c2496da0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'value': '5ac9eb49ff80ec55c2e1e86fb5eea307624d77a3da4fd48e517bac8512410142'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Start the llama2 70B model\n",
        "together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41q7h-Pyn4Gf"
      },
      "outputs": [],
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    # @root_validator()\n",
        "    # def validate_environment(cls, values: Dict) -> Dict:\n",
        "    #     \"\"\"Validate that the API key is set.\"\"\"\n",
        "    #     api_key = get_from_dict_or_env(\n",
        "    #         values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "    #     )\n",
        "    #     values[\"together_api_key\"] = api_key\n",
        "    #     return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tp5EKVuoGDj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"\\n<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBtZm1VGoMyv"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkHV0CbXqQlV",
        "outputId": "e92470fa-34a5-4052-b70a-003ae3ec2c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]\n",
            "<<SYS>>\n",
            "\n",
            "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
            "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
            "  You must answer the question based on only chat messages you are given.\n",
            "  Don't answer anything outside the context you are provided and do not respond with anything from your general knowledge.\n",
            "  Try to mention the ones that you get the context from.\n",
            "  You may also look at the chat history to get additional context if necessary.\n",
            "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "### Chat Messages (Context): \n",
            "\n",
            "{context} \n",
            "\n",
            "\n",
            "\n",
            "### Chat History: \n",
            "\n",
            "{chat_history} \n",
            "Human: {user_input} \n",
            "Assistant:\n",
            "[/INST]\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "instruction = \"\"\"\n",
        "### Chat Messages (Context): \\n\\n{context} \\n\\n\\n\n",
        "### Chat History: \\n\\n{chat_history} \\nHuman: {user_input} \\nAssistant:\\n\"\"\"\n",
        "system_prompt = \"\"\"\n",
        "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
        "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
        "  You must answer the question based on only chat messages you are given.\n",
        "  Don't answer anything outside the context you are provided and do not respond with anything from your general knowledge.\n",
        "  Try to mention the ones that you get the context from.\n",
        "  You may also look at the chat history to get additional context if necessary.\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "  \"\"\"\n",
        "  # You may also read the chat history to get additional context\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_input\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "uJwKcvOD_el3",
        "outputId": "e6196ab1-8c18-491c-95c6-266dfb8b58b8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hi Sam, I\\'m happy to help! Based on the chat messages you provided, it seems like Alice and Bob were discussing the meaning of the abbreviation \"LLM.\" Bob explained that it stands for \"Large Language Model.\" Is there anything else you would like to know or discuss?'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.predict(context=\"Alice: What's an LLM? \\n Bob: It's an abbreviation for Large Langage Model.\", user_input=\"Hi, my name is Sam\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jO1dykn099_g",
        "outputId": "380ee760-5c61-40f5-ff25-2d4f348a9554"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Assistant: Hi Sam, based on the chat messages you provided, LLM stands for Large Language Model. It was explained by Bob in the chat.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.predict(context=\"Alice: What's an LLM? \\nBob: It's an abbreviation for Large Langage Model. \\nAlice: Oh, I see. Can you give me some examples of LLMs? \\nBob: Sure, there's GPT4, Liama and Palms\", user_input=\"What does LLM stand for?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ep7nEiZA_efr",
        "outputId": "e118e9f7-4475-4899-dfa0-0891b0bb39d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Sure, based on the chat messages you provided, Bob gave some examples of LLMs, which are: GPT4, Liama, Palms, RoBerta, Shanon, and Transfirmers.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.predict(context=\"Alice: What's an LLM? \\nBob: It's an abbreviation for Large Langage Model. \\nAlice: Oh, I see. Can you give me some examples of LLMs? \\nBob: Sure, there's GPT4, Liama, Palms, RoBerta, Shanon and Transfirmers\", user_input=\"Can you give me some more examples?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6H8sH-az_v04",
        "outputId": "931601a8-bd23-4a5a-9609-f67996eb09fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Assistant: Hi Sam, based on the chat messages you provided, your name was not explicitly mentioned. However, I can see that you were the one who initiated the conversation and asked questions about LLMs. Therefore, I can infer that your name is Sam. Is there anything else you would like to know or discuss?'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# llm_chain.predict(context=\"Alice: I'm Alice. \\nJohn: Nice to meet you Alice, My name is John.\", user_input=\"What's my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Wl6es6ei35"
      },
      "outputs": [],
      "source": [
        "\n",
        "########################################\n",
        "\n",
        "def semantic_search(query):\n",
        "  data = get_data_from_chroma(query)\n",
        "\n",
        "  context = data['context']\n",
        "  metadata = data['metadata']\n",
        "\n",
        "  response = llm_chain.predict(context=context, user_input=query)\n",
        "\n",
        "  return {'response': str(response), 'metadata': metadata}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_-udqH5l0GA",
        "outputId": "66e7d6d4-3199-4991-a19d-df5a70e81037"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding query ...\n",
            "CPU times: user 311 ms, sys: 21.4 ms, total: 332 ms\n",
            "Wall time: 16.4 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': \" Sure, I'd be happy to help! Here's a simple recipe for making pancakes:\\n\\nIngredients:\\n\\n* 1 cup all-purpose flour\\n* 2 tablespoons sugar\\n* 2 teaspoons baking powder\\n* 1/4 teaspoon salt\\n* 1 cup milk\\n* 1 egg\\n* 2 tablespoons butter, melted\\n* Butter or oil for greasing the pan\\n\\nInstructions:\\n\\n1. In a large bowl, whisk together the flour, sugar, baking powder, and salt.\\n2. In a separate bowl, whisk together the milk, egg, and melted butter.\\n3. Pour the wet ingredients into the dry ingredients and stir until just combined. The batter should still be slightly lumpy.\\n4. Heat a non-stick pan or griddle over medium heat. Grease the pan with butter or oil.\\n5. Using a 1/4 cup measuring cup, scoop the batter onto the pan.\\n6. Cook the pancakes for 2-3 minutes, until bubbles appear on the surface and the edges start to dry.\\n7. Flip the pancakes and cook for another 1-2 minutes, until golden brown.\\n8. Serve the pancakes hot with your favorite toppings, such as syrup, butter, fruit, or whipped cream.\\n\\nI hope this helps! Let me know if you have any questions or if you'd like any variations on the recipe.\",\n",
              " 'metadata': [{'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: <https://haystack.deepset.ai/tutorials/08_preprocessing>',\n",
              "   'time': '1687166202.864639',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.24070346355438232},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Bzw I already tries this a while ago but forgot to share this is what the export looks like',\n",
              "   'time': '1687188005.017139',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.19655275344848633},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: And we should also post some stickers... :grinning: :smile: :grin:',\n",
              "   'time': '1687169124.658349',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.16383564472198486},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Sarah: I've been using GPT2 for similar tasks, and it has been working well for me.\",\n",
              "   'time': '1687177702.175079',\n",
              "   'user_id': 'U03ABCDE03',\n",
              "   'user_name': 'Sarah Davis',\n",
              "   'score': 0.1533496379852295},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Alice: If you're looking for a more lightweight alternative, you can consider DistilBERT. It's a distilled version of BERT that offers faster inference times.\",\n",
              "   'time': '1687177717.175079',\n",
              "   'user_id': 'U02ABCDE02',\n",
              "   'user_name': 'Alice Johnson',\n",
              "   'score': 0.14602279663085938},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Sarah: GPT3 has set the bar high, but we should keep an eye on research papers and see if any promising alternatives are being developed.',\n",
              "   'time': '1687177706.175079',\n",
              "   'user_id': 'U03ABCDE03',\n",
              "   'user_name': 'Sarah Davis',\n",
              "   'score': 0.1416149139404297},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Sarah: BERT is a widely-used model that can serve as a good alternative to GPT3. It's particularly effective for tasks involving natural language understanding.\",\n",
              "   'time': '1687177718.175079',\n",
              "   'user_id': 'U03ABCDE03',\n",
              "   'user_name': 'Sarah Davis',\n",
              "   'score': 0.13156956434249878},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: yeah then we’ll see how we can clean the data',\n",
              "   'time': '1687167171.439409',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.1229928731918335},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: hello',\n",
              "   'time': '1687166197.580079',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.1160019040107727},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-07-01',\n",
              "   'message': \"(2023-07-01) Alice: Me too! I'm already considering implementing PWAs for my website to enhance user engagement.\",\n",
              "   'time': '1687180140.175118',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'Alice Johnson',\n",
              "   'score': 0.1101754903793335},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: Also, welcome <@U05D4M7RGQ3>',\n",
              "   'time': '1687169174.199539',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.10278129577636719},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Good work.. now we don't have to worry about exporting data.\",\n",
              "   'time': '1687166814.786429',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.10022276639938354},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: but we can’t use it for search since it’ll be very small',\n",
              "   'time': '1687167197.595189',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.09406942129135132},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Sarah: I heard OpenAI is working on GPT4, which might offer significant improvements over GPT3. We should keep an eye on their advancements.',\n",
              "   'time': '1687177710.175079',\n",
              "   'user_id': 'U03ABCDE03',\n",
              "   'user_name': 'Sarah Davis',\n",
              "   'score': 0.09277468919754028},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Alice: Absolutely. BERT's smaller size allows for faster training and deployment, which is crucial for our application's performance.\",\n",
              "   'time': '1687177736.175079',\n",
              "   'user_id': 'U02ABCDE02',\n",
              "   'user_name': 'Alice Johnson',\n",
              "   'score': 0.08790201915012741},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is a good book for design nerds like me',\n",
              "   'time': '1687177700.175079',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.08664417266845703},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Alice: BERT is another popular model that could be considered as an alternative to GPT3. It has been widely used for various NLP tasks.',\n",
              "   'time': '1687177709.175079',\n",
              "   'user_id': 'U02ABCDE02',\n",
              "   'user_name': 'Alice Johnson',\n",
              "   'score': 0.08560812473297119},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: It's best if we just post random topics here to test the semantic search.\",\n",
              "   'time': '1687166901.338569',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.08257949352264404},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Just for test case we should add some picture to see how slack handles it',\n",
              "   'time': '1687177638.199629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.08159065246582031},\n",
              "  {'channel': 'gptgenerated',\n",
              "   'date': '2023-07-01',\n",
              "   'message': \"(2023-07-01) Yeabesera: Indeed! And with the ability to add to the home screen, PWAs provide a seamless integration with the user's device.\",\n",
              "   'time': '1687180120.175116',\n",
              "   'user_id': 'U05D4M7RGQ3',\n",
              "   'user_name': 'Yeabesera Derese',\n",
              "   'score': 0.07768470048904419}]}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "%%time\n",
        "# semantic_search(\"Hello, my name is John.\")\n",
        "# semantic_search(\"What did Tollan say about semantic search?\")\n",
        "# semantic_search(\"What are some models that are comparable to GPT 3?\")\n",
        "semantic_search(\"How can I make some pancakes?\")\n",
        "# semantic_search(\"What's my name?\")\n",
        "# semantic_search(\"Alright, Thanks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB-U-Yxjp4Vx"
      },
      "source": [
        "## 7. Creating an API Endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DeAXHDCkz5v",
        "outputId": "9b449f93-61e1-4663-fffe-9298112cb0eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-09-04T06:33:13+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public url for the API... https://b39a-34-16-185-172.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "port_no = 5000\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "ngrok_auth_token = os.environ[\"NGROK_AUTH_TOKEN\"]\n",
        "\n",
        "ngrok.set_auth_token( ngrok_auth_token )\n",
        "public_url =  ngrok.connect(port_no).public_url\n",
        "\n",
        "@app.route(\"/\", methods=['GET', 'POST'])\n",
        "def semantic_search_query():\n",
        "\n",
        "  if request.method == 'GET':\n",
        "    query = request.args.get('query')\n",
        "    return semantic_search(query)\n",
        "\n",
        "  elif request.method == 'POST':\n",
        "    query = request.json['query']\n",
        "    return semantic_search(query)\n",
        "\n",
        "\n",
        "print(f\"Public url for the API... {public_url}\")\n",
        "\n",
        "app.run(port=port_no)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmlOIW8yAPJo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}