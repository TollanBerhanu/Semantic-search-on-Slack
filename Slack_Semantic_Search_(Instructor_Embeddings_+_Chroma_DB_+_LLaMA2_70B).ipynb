{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TollanBerhanu/Semantic-search-on-Slack/blob/main/Slack_Semantic_Search_(Instructor_Embeddings_%2B_Chroma_DB_%2B_LLaMA2_70B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Semantic Search on Exported Slack Data"
      ],
      "metadata": {
        "id": "I99shZMWoyOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains an implementation of semantic search on exported slack data\n",
        "\n",
        "*   *This notebook utilizes the following tools:*\n",
        "\n",
        ">\n",
        "\n",
        "    1.   'Pandas' - to load and extract relevant information from the exported data\n",
        "    2.   'Instructor embedding model' - to generate embeddings for each message\n",
        "    3.   'Chroma' - to store and query the vector embeddings along with some metadata\n",
        "    4.   'LLaMA2 7B model' - to present the results in natural language\n"
      ],
      "metadata": {
        "id": "xwobrIb-oZjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**API** Keys"
      ],
      "metadata": {
        "id": "sciMCYuJHB8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"ac17a88fb15afc19f632fc58d39d177814f3ead1d013f7adc9bce9f3ccf33580\"\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = \"2UKtqNC7pDrDKG272UqIOy4rvSm_2ezkSxzZ7LDUBey1S2dM6\""
      ],
      "metadata": {
        "id": "12dbz_yAGrTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing dependencies"
      ],
      "metadata": {
        "id": "Zx9_YuP5oJwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gitpython to clone a github repo containing the exported slack data\n",
        "!pip install gitpython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWwTMsTDhZ1",
        "outputId": "264b9d96-035e-47cd-eeb8-17d5b8f95d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.32 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Langchain and Chroma\n",
        "!pip -q install langchain chromadb"
      ],
      "metadata": {
        "id": "11lUKMYtmQ7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ngrok to host an api endpoint from colab\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "BKYJVueRmJkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edadfe22-a09c-4628-ecae-ee713a9bd0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-6.0.0.tar.gz (681 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.2/681.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-6.0.0-py3-none-any.whl size=19867 sha256=8f7ebf90c8a696b3d43789f61aa4099125e6193e4fda17ea99fbd07e5222c793\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/42/78/0c3d438d7f5730451a25f7ac6cbf4391759d22a67576ed7c2c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install flask cors to enable cors for all domains.\n",
        "!pip install -U flask-cors"
      ],
      "metadata": {
        "id": "tV7SvQwG6Ilu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635da962-7087-48fb-fe2c-3c5145a06f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.3)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fetching the slack data"
      ],
      "metadata": {
        "id": "28aIsFkVoM_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path where the exported slack data is stored in local storage\n",
        "slack_data_path = \"/content/slackdata/\""
      ],
      "metadata": {
        "id": "FjhyBDOmoeAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this if you haven't already cloned the GitHub repository"
      ],
      "metadata": {
        "id": "elPjfiLOnQfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import git\n",
        "\n",
        "repo_url = \"https://github.com/TollanBerhanu/MatterMost-LLM-test-Slack-export-Jun-19-2023---Jun-20-2023.git\"\n",
        "# slack_data_path = '/content/drive/MyDrive/Colab Notebooks/dataset/slack-data/'\n",
        "\n",
        "git.Repo.clone_from(repo_url, slack_data_path)"
      ],
      "metadata": {
        "id": "mnp8_O_4DiuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47412ec8-c023-4882-d141-b569b80ffd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<git.repo.base.Repo '/content/slackdata/.git'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_channels(path):\n",
        "  df = pd.read_json(path + 'channels.json')\n",
        "\n",
        "  channel_ids = [id for id in df['id']]\n",
        "  channel_names = [ name for name in df['name']]\n",
        "\n",
        "  return pd.DataFrame({ 'channel_id': channel_ids, 'channel_name': channel_names } )\n",
        "\n",
        "channels = get_all_channels(slack_data_path)\n",
        "channels"
      ],
      "metadata": {
        "id": "l6XPLPVrqEaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "ec2456b6-13c1-4aac-b9d9-3e7c3560606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    channel_id  channel_name\n",
              "0  C05D1SE01B7        random\n",
              "1  C05D77W3N76       general\n",
              "2  C05D7863DRA          test\n",
              "3   C05ABCDE01  gptgenerated"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41cf223b-feda-479d-95a9-adfd546d43f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>channel_id</th>\n",
              "      <th>channel_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C05D1SE01B7</td>\n",
              "      <td>random</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C05D77W3N76</td>\n",
              "      <td>general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C05D7863DRA</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C05ABCDE01</td>\n",
              "      <td>gptgenerated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41cf223b-feda-479d-95a9-adfd546d43f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-41cf223b-feda-479d-95a9-adfd546d43f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-41cf223b-feda-479d-95a9-adfd546d43f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8af64827-df50-417b-b18a-8237a4d36777\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8af64827-df50-417b-b18a-8237a4d36777')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8af64827-df50-417b-b18a-8237a4d36777 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import json\n",
        "\n",
        "# Return the metadata of each message in the channel\n",
        "def extract_channel_metadata(path, channel_name):\n",
        "  try:\n",
        "    daily_json_files = glob.glob(path + channel_name +'/*.json')  # use glob to get all the json files in the folder\n",
        "  except:\n",
        "    print(f'The channel name {channel_name} is invalid!')\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "  if not daily_json_files:  # return if the channel doesn't exist (or hasn't been exported yet)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "  metadata = pd.DataFrame(columns = ['message', 'channel', 'date', 'time', 'user_id', 'user_name'])\n",
        "\n",
        "  # loop over the list of json files (each json file includes every message in that channel for a single day)\n",
        "  for f in daily_json_files:\n",
        "    with open(f, 'r') as file:  # open the daily json file\n",
        "        data = file.read()  # Read the contents\n",
        "        today_data = json.loads(data) # Parse the JSON data\n",
        "\n",
        "    today_date = f.split(\"/\")[-1]  # 'f' is the full file path and file name\n",
        "    print('Extracting...', today_date) # the file name is the date\n",
        "\n",
        "    # iterate through all the messages of the day\n",
        "    for msg_data in today_data:\n",
        "      # Skip if its a \"channel_join\" type message or if the actual message content is empty\n",
        "      if ('subtype' in msg_data) or (msg_data['text'] == \"\") or (msg_data['type'] != 'message'):\n",
        "        continue\n",
        "        # TODO: filter out any links, stickers, and other junk\n",
        "        # TODO: replace @Member references with their real names\n",
        "\n",
        "      metadata.loc[len(metadata)] = {\n",
        "            'message': '(' + today_date.split(\".json\")[0] + ') ' + msg_data['user_profile']['first_name'] + ': ' + msg_data['text'],\n",
        "            'channel': channel_name,\n",
        "            'date': today_date.split(\".json\")[0], # omit the file extension '.json'\n",
        "            'time': msg_data['ts'],\n",
        "            'user_id': msg_data['user'],\n",
        "            'user_name': msg_data['user_profile']['real_name'] # We can use 'first_name' to get the first name and 'real_name' to get the full name of the user\n",
        "      }\n",
        "\n",
        "  return metadata\n",
        "\n",
        "# extract_channel_metadata(slack_data_path, 'test')"
      ],
      "metadata": {
        "id": "bXruD2kzE7Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Generating Embeddings"
      ],
      "metadata": {
        "id": "hCa_VB44FBlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import ast\n",
        "\n",
        "url = 'https://hackingfaces.onrender.com/embed'\n",
        "embedding_model_url = 'https://huggingface.co/spaces/tollan/instructor-xl'\n",
        "# embedding_model_url = 'https://huggingface.co/spaces/tollan/sentence-transformers-embedding'\n",
        "\n",
        "# Get the list of embeddings for all messages in a channel\n",
        "def embed_channel_messages(channel_messages):\n",
        "  msg_list = channel_messages.astype(str).tolist()\n",
        "  post_data = {\n",
        "            'link': embedding_model_url ,\n",
        "            # 'query': \"['hi','hello']\"\n",
        "            'query': str(msg_list)\n",
        "          }\n",
        "\n",
        "  embeddings = requests.post(url, data = post_data, headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"})\n",
        "\n",
        "  return ast.literal_eval(embeddings.text)\n",
        "\n",
        "# Get the corresponding embedding for the user's query\n",
        "def embed_query(query):\n",
        "  post_data = {\n",
        "            'link': embedding_model_url ,\n",
        "            # 'query': \"['hi','hello']\"\n",
        "            'query': str([query])\n",
        "          }\n",
        "\n",
        "  embeddings = requests.post(url, data = post_data, headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}, timeout=120)\n",
        "\n",
        "  return ast.literal_eval(embeddings.text)[0]"
      ],
      "metadata": {
        "id": "hCqYnwcXUe23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Storing the embeddings in Chroma DB\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb"
      ],
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = chromadb.PersistentClient(path=\"/content/chroma_db\")"
      ],
      "metadata": {
        "id": "3m5iAQbhXfNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a collection object from an existing collection, by name. If it doesn't exist, create one.\n",
        "collection = client.get_or_create_collection(\n",
        "      name= \"slack_collection\",\n",
        "      metadata= {\"hnsw:space\": \"cosine\"},\n",
        "    )"
      ],
      "metadata": {
        "id": "HASSWsxJXg1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning: Delete a collection and all associated embeddings, documents, and metadata. ⚠️ This is destructive and not reversible :(\n",
        "# client.delete_collection(name=\"slack_collection\")"
      ],
      "metadata": {
        "id": "llBVxIWRYeoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_channel_embeddings(channel_embeddings, channel_metadata):\n",
        "\n",
        "  # parse the channel metadata to json\n",
        "  parsed_channel_metadata = json.loads(channel_metadata.to_json(orient=\"records\"))\n",
        "\n",
        "  # create IDs for the embeddings\n",
        "  # ids = [ (channel_name + str(ch)) for ch in enumerate(channel_embeddings) ] ... [channelname_0 -> ... -> channelname_n]\n",
        "  ids = [ str(hash(metadata['message'])) for metadata in parsed_channel_metadata ]\n",
        "  try:\n",
        "  # upsert the embeddings along with their metadata, into a Chroma collection\n",
        "    collection.upsert(\n",
        "      ids = ids,\n",
        "      embeddings = channel_embeddings,\n",
        "      metadatas = parsed_channel_metadata,\n",
        "      # documents = channel_metadata['channel'].astype(str).tolist()\n",
        "    )\n",
        "  except chromadb.errors.DuplicateIDError as duplicate_err:\n",
        "    print(f'This one exists already: {duplicate_err}')\n",
        "\n",
        "  # print(collection.peek()) # returns a list of the first 10 items in the collection\n",
        "  print(f'Total items in collection: { collection.count() }') # returns the number of items in the collection"
      ],
      "metadata": {
        "id": "Baf3JQpTevBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "step = 15\n",
        "\n",
        "# Upsert channel's data to the vector db\n",
        "def upsert_channels(channel_names=[]):\n",
        "  if (channel_names == []):\n",
        "    channel_names = channels['channel_name'].tolist()\n",
        "\n",
        "  for idx, ch_name in enumerate(channel_names):\n",
        "    print(f'Upserting channel { str(idx+1) } of { str(len(channel_names)) }: \"{ch_name}\" ... ')\n",
        "\n",
        "    channel_metadata = extract_channel_metadata(slack_data_path, ch_name)\n",
        "\n",
        "    if (channel_metadata.empty):\n",
        "      print('-> The channel is empty / doesn\\'t exist!')\n",
        "      continue\n",
        "\n",
        "    no_messages = len(channel_metadata)\n",
        "\n",
        "    for start in range(0, no_messages, step):\n",
        "\n",
        "      end = min(no_messages, start+step)\n",
        "      channel_metadata_batch = channel_metadata[start:end]\n",
        "\n",
        "      print(f'-> Embedding Batch { math.ceil(end/step) }/{ math.ceil(no_messages/step) } ...')\n",
        "      # print(str(channel_metadata_batch['message']))\n",
        "\n",
        "      channel_embeddings = embed_channel_messages(channel_metadata_batch['message'])\n",
        "\n",
        "      upsert_channel_embeddings(channel_embeddings[start:end], channel_metadata_batch)"
      ],
      "metadata": {
        "id": "xyvh1D1zSLTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upsert_channels() # upsert all channels\n",
        "upsert_channels(['random', 'test', 'general'])  # general, random, gptgenerated"
      ],
      "metadata": {
        "id": "-T18gCFHTKWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0709405-f0c1-4cfb-a173-2f99b2cbd847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserting channel 1 of 3: \"random\" ... \n",
            "Extracting... 2023-06-19.json\n",
            "-> Embedding Batch 1/1 ...\n",
            "Total items in collection: 1\n",
            "Upserting channel 2 of 3: \"test\" ... \n",
            "Extracting... 2023-06-19.json\n",
            "The channel is empty or it doesn't exist!\n",
            "Upserting channel 3 of 3: \"general\" ... \n",
            "Extracting... 2023-06-19.json\n",
            "-> Embedding Batch 1/1 ...\n",
            "Total items in collection: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the persisted database from disk, and use it as normal.\n",
        "# vectordb = Chroma(persist_directory= '/content/chroma_db')"
      ],
      "metadata": {
        "id": "A-h1y_eAHmD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Querying the messages from Chroma"
      ],
      "metadata": {
        "id": "Zp4z2dBnplNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_from_chroma(query):\n",
        "  # Generate embeddings for the query\n",
        "  print('Embedding query ...')\n",
        "  embedded_query = embed_query(query)\n",
        "\n",
        "  query_response = collection.query(\n",
        "      query_embeddings = embedded_query,\n",
        "      n_results = 20,\n",
        "      # where = {\"metadata_field\": \"is_equal_to_this\"},\n",
        "      where = {\n",
        "          # \"channel\": {\"$eq\": \"general\"}\n",
        "          # \"user_id\": {\"$in\": [\"U05D1SQDNSH\", \"U05DHDPL4FK\", \"U05CQ93C3FZ\", \"U05D4M7RGQ3\"]}\n",
        "      }\n",
        "      # where_document={\"$contains\":\"search_string\"}\n",
        "  )\n",
        "\n",
        "  # documents = query_response['documents']\n",
        "  scores = query_response['distances'][0]\n",
        "  metadatas = query_response['metadatas'][0]\n",
        "\n",
        "  context = ''\n",
        "\n",
        "  for idx, metadata in enumerate(query_response['metadatas'][0]):\n",
        "    context += metadata['message'] + '\\n'\n",
        "    metadata['score'] = 1 - scores[idx]\n",
        "\n",
        "  return {'context': context, 'metadata': metadatas}\n",
        "\n",
        "# get_data_from_chroma(\"Why was it good work?\")\n",
        "get_data_from_chroma(\"What did Tollan say was good work?\")\n",
        "# get_data_from_chroma(\"What are some models that are comparable to GPT 3?\")"
      ],
      "metadata": {
        "id": "7h7hrBSYk40W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbd1ea9-ff0a-414c-db48-e25dd7f4da70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding query ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"(2023-06-19) Tollan: Good work.. now we don't have to worry about exporting data.\\n(2023-06-19) Tollan: Also, welcome <@U05D4M7RGQ3>\\n(2023-06-19) Tollan: And we should also post some stickers... :grinning: :smile: :grin:\\n(2023-06-19) Tollan: It's best if we just post random topics here to test the semantic search.\\n(2023-06-19) Tollan: Oh.. u r right.. the search probably won't work properly with a small amount of data.\\n(2023-06-19) Eyob: This is interesting\\n(2023-06-19) Eyob: This is a good book for design nerds like me\\n(2023-06-19) Eyob: Sample audio as well\\n(2023-06-19) kenenisaalemayhu0: yeah then we’ll see how we can clean the data\\n(2023-06-19) Eyob: Just for test case we should add some picture to see how slack handles it\\n(2023-06-19) Eyob: Bzw I already tries this a while ago but forgot to share this is what the export looks like\\n(2023-06-19) kenenisaalemayhu0: <https://haystack.deepset.ai/tutorials/08_preprocessing>\\n(2023-06-19) kenenisaalemayhu0: Random shiii\\n(2023-06-19) kenenisaalemayhu0: but we can’t use it for search since it’ll be very small\\n(2023-06-19) Eyob: I didn't know slack had a dark mode until I saw the screenshot\\n(2023-06-19) kenenisaalemayhu0: hello\\n\",\n",
              " 'metadata': [{'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Good work.. now we don't have to worry about exporting data.\",\n",
              "   'time': '1687166814.786429',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.7387857805279638},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: Also, welcome <@U05D4M7RGQ3>',\n",
              "   'time': '1687169174.199539',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.7043737201457052},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: And we should also post some stickers... :grinning: :smile: :grin:',\n",
              "   'time': '1687169124.658349',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.6640573370321882},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: It's best if we just post random topics here to test the semantic search.\",\n",
              "   'time': '1687166901.338569',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.6529905700841084},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Oh.. u r right.. the search probably won't work properly with a small amount of data.\",\n",
              "   'time': '1687168952.621439',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.6175846086535836},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is interesting',\n",
              "   'time': '1687177484.530639',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.609082071076609},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is a good book for design nerds like me',\n",
              "   'time': '1687177700.175079',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.6075226982777642},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Sample audio as well',\n",
              "   'time': '1687178138.528629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.5692409975843563},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: yeah then we’ll see how we can clean the data',\n",
              "   'time': '1687167171.439409',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.5608013475511525},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Just for test case we should add some picture to see how slack handles it',\n",
              "   'time': '1687177638.199629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.5533184041679751},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Bzw I already tries this a while ago but forgot to share this is what the export looks like',\n",
              "   'time': '1687188005.017139',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.5342647671653987},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: <https://haystack.deepset.ai/tutorials/08_preprocessing>',\n",
              "   'time': '1687166202.864639',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.5296506697891717},\n",
              "  {'channel': 'random',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: Random shiii',\n",
              "   'time': '1687166237.151359',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.5259865965861772},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: but we can’t use it for search since it’ll be very small',\n",
              "   'time': '1687167197.595189',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.5210743629556008},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Eyob: I didn't know slack had a dark mode until I saw the screenshot\",\n",
              "   'time': '1687177520.385329',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.518341532340431},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: hello',\n",
              "   'time': '1687166197.580079',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.5130908893055259}]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Getting the response from LLaMA 2 - 7B"
      ],
      "metadata": {
        "id": "siLXR-XT0JoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install huggingface_hub tiktoken\n",
        "!pip -q install --upgrade together"
      ],
      "metadata": {
        "id": "E0lALSEbYVbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "# set your API key\n",
        "together_api_key = os.environ[\"TOGETHER_API_KEY\"]\n",
        "together.api_key = together_api_key\n",
        "\n",
        "# list available models and descriptons\n",
        "models = together.Models.list()\n",
        "\n",
        "# print the first model's name\n",
        "print(models[3]['name']), print(models[52]['name'])\n",
        "# List all available models\n",
        "# for idx, model in enumerate(models):\n",
        "#     print(idx, model['name'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J4PBG2SYdDr",
        "outputId": "65eb066b-1dde-4e71-9403-5b3b7a0bc0b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EleutherAI/pythia-1b-v0\n",
            "togethercomputer/llama-2-70b-chat\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the llama2 70B model\n",
        "together.Models.start(\"togethercomputer/llama-2-70b-chat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ5aHi9FYhU7",
        "outputId": "faa5c4df-d754-465a-a5e3-1dd141a8f850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'value': 'c0b42cda581be41063c9e8b1c4bbebbf535972afd3874996496ba049eb7009f4'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    # @root_validator()\n",
        "    # def validate_environment(cls, values: Dict) -> Dict:\n",
        "    #     \"\"\"Validate that the API key is set.\"\"\"\n",
        "    #     api_key = get_from_dict_or_env(\n",
        "    #         values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "    #     )\n",
        "    #     values[\"together_api_key\"] = api_key\n",
        "    #     return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "41q7h-Pyn4Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"\\n<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ],
      "metadata": {
        "id": "8Tp5EKVuoGDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "RBtZm1VGoMyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "instruction = \"\"\"\n",
        "### Chat Messages (Context): \\n\\n{context} \\n\\n\\n\n",
        "### Chat History: \\n\\n{chat_history} \\nHuman: {user_input} \\nAssistant:\\n\"\"\"\n",
        "system_prompt = \"\"\"\n",
        "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
        "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
        "  You must answer the question based on only chat messages you are given.\n",
        "  Don't answer anything outside the context you are provided and do not respond with anything from your general knowledge.\n",
        "  Try to mention the ones that you get the context from.\n",
        "  You may also look at the chat history to get additional context if necessary.\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "  \"\"\"\n",
        "  # You may also read the chat history to get additional context\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"chat_history\", \"user_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"user_input\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "mkHV0CbXqQlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c10a55-747d-480c-cd40-38d050ab4a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]\n",
            "<<SYS>>\n",
            "\n",
            "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
            "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
            "  Do not mention anything outside of what is discussed. Don't answer anything outside the context you are provided and don't add anything from your prevoius knowledge.\n",
            "  Try to mention the ones that you get the context from.\n",
            "  You may also look at the chat history to get additional context if necessary.\n",
            "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "### Chat Messages (Context): \n",
            "\n",
            "{context} \n",
            "\n",
            "\n",
            "\n",
            "### Chat History: \n",
            "\n",
            "{chat_history} \n",
            "Human: {user_input} \n",
            "Assistant:\n",
            "[/INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: What's an LLM? \\n Bob: It's an abbreviation for Large Langage Model.\", user_input=\"Hi, my name is Sam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "uJwKcvOD_el3",
        "outputId": "1644b6c6-c7db-4319-f73e-9c4cbfb01d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]\n",
            "<<SYS>>\n",
            "\n",
            "  You are a helpful assistant, you always only answer for the assistant then you stop. You will answer the question the Human asks.\n",
            "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
            "  Do not mention anything outside of what is discussed. Don't answer anything outside the context you are provided. \n",
            "  You may also look at the chat history to get additional context if necessary.\n",
            "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "### Chat Messages (Context): \n",
            "\n",
            "Alice: What's an LLM? \n",
            " Bob: It's an abbreviation for Large Langage Model. \n",
            "\n",
            "\n",
            "\n",
            "### Chat History: \n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hi Sam, my name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. How can I assist you today? \n",
            "Human: Hi, my name is Sam \n",
            "Assistant:\n",
            "[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi Sam, my name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: What's an LLM? \\nBob: It's an abbreviation for Large Langage Model. \\nAlice: Oh, I see. Can you give me some examples of LLMs? \\nBob: Sure, there's GPT4, Liama and Palms\", user_input=\"What does LLM stand for?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "jO1dykn099_g",
        "outputId": "e0b63866-24ab-4832-84c1-7889a656ba0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]\n",
            "<<SYS>>\n",
            "\n",
            "  You are a helpful assistant, you always only answer for the assistant then you stop. You will answer the question the Human asks.\n",
            "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
            "  Do not mention anything outside of what is discussed. Don't answer anything outside the context you are provided. \n",
            "  You may also look at the chat history to get additional context if necessary.\n",
            "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "### Chat Messages (Context): \n",
            "\n",
            "Alice: What's an LLM? \n",
            "Bob: It's an abbreviation for Large Langage Model. \n",
            "Alice: Oh, I see. Can you give me some examples of LLMs? \n",
            "Bob: Sure, there's GPT4, Liama and Palms \n",
            "\n",
            "\n",
            "\n",
            "### Chat History: \n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hi Sam, my name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. How can I assist you today?\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hi Sam, my name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. How can I assist you today? \n",
            "Human: What does LLM stand for? \n",
            "Assistant:\n",
            "[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Assistant: LLM stands for Large Language Model. It's a type of artificial intelligence model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. Examples of LLMs include GPT4, Liama, and Palms.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: What's an LLM? \\nBob: It's an abbreviation for Large Langage Model. \\nAlice: Oh, I see. Can you give me some examples of LLMs? \\nBob: Sure, there's GPT4, Liama, Palms, RoBerta, Shanon and Transfirmers\", user_input=\"Can you give me some more examples?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "Ep7nEiZA_efr",
        "outputId": "f2557c5b-d86e-47c0-bd87-f0299273bb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]\n",
            "<<SYS>>\n",
            "\n",
            "  You are a helpful assistant, you always only answer for the assistant then you stop. You will answer the question the Human asks.\n",
            "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
            "  Do not mention anything outside of what is discussed. Don't answer anything outside the context you are provided. \n",
            "  You may also look at the chat history to get additional context if necessary.\n",
            "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "### Chat Messages (Context): \n",
            "\n",
            "Alice: What's an LLM? \n",
            "Bob: It's an abbreviation for Large Langage Model. \n",
            "Alice: Oh, I see. Can you give me some examples of LLMs? \n",
            "Bob: Sure, there's GPT4, Liama, Palms, RoBerta, Shanon and Transfirmers \n",
            "\n",
            "\n",
            "\n",
            "### Chat History: \n",
            "\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hi Sam, my name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. How can I assist you today?\n",
            "Human: Hi, my name is Sam\n",
            "AI:  Hi Sam, my name is LLaMA, I'm a large language model trained by a team of researcher at Meta AI. How can I assist you today?\n",
            "Human: What does LLM stand for?\n",
            "AI:  Assistant: LLM stands for Large Language Model. It's a type of artificial intelligence model that is trained on a large corpus of text data to generate language outputs that are coherent and natural-sounding. Examples of LLMs include GPT4, Liama, and Palms. \n",
            "Human: Can you give me some more examples? \n",
            "Assistant:\n",
            "[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sure, here are some more examples of LLMs: RoBerta, Shannon, and Transformers. These models are all designed to process and generate human-like language, and they have been used in a variety of applications such as chatbots, language translation, and text summarization.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(context=\"Alice: I'm Alice. \\nJohn: Nice to meet you Alice, My name is John.\", user_input=\"What's my name?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "6H8sH-az_v04",
        "outputId": "a8523fe2-7cd7-4496-a049-2df1193acd8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m[INST]\n",
            "<<SYS>>\n",
            "\n",
            "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
            "  You will be given a sequence of chat messages related to a certain topic. Write a response that answers the question based on what is discussed in the chat messages.\n",
            "  Do not mention anything outside of what is discussed. Don't answer anything outside the context you are provided and don't add anything from your prevoius knowledge.\n",
            "  Try to mention the ones that you get the context from. \n",
            "  You may also look at the chat history to get additional context if necessary.\n",
            "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
            "  \n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "### Chat Messages (Context): \n",
            "\n",
            "Alice: I'm Alice. \n",
            "John: Nice to meet you Alice, My name is John. \n",
            "\n",
            "\n",
            "\n",
            "### Chat History: \n",
            "\n",
            " \n",
            "Human: What's my name? \n",
            "Assistant:\n",
            "[/INST]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Based on the chat messages provided, your name is John.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########################################\n",
        "\n",
        "def semantic_search(query):\n",
        "  data = get_data_from_chroma(query)\n",
        "\n",
        "  context = data['context']\n",
        "  metadata = data['metadata']\n",
        "\n",
        "  response = llm_chain.predict(context=context, user_input=query)\n",
        "\n",
        "  return {'response': str(response), 'metadata': metadata}"
      ],
      "metadata": {
        "id": "x1Wl6es6ei35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# semantic_search(\"Hello, my name is John.\")\n",
        "# semantic_search(\"What did Tollan say about semantic search?\")\n",
        "# semantic_search(\"What are some models that are comparable to GPT 3?\")\n",
        "semantic_search(\"How can I make some pancakes?\")\n",
        "# semantic_search(\"What's my name?\")\n",
        "# semantic_search(\"Alright, Thanks!\")"
      ],
      "metadata": {
        "id": "m_-udqH5l0GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f3d42e-8524-42e1-fa4a-7ec9df24f508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding query ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 509 ms, sys: 50.1 ms, total: 559 ms\n",
            "Wall time: 42.9 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'response': \" Sure, I can help you with that! Here's a simple recipe for making pancakes:\\n\\nIngredients:\\n\\n* 1 cup all-purpose flour\\n* 2 tablespoons sugar\\n* 2 teaspoons baking powder\\n* 1/4 teaspoon salt\\n* 1 cup milk\\n* 1 egg\\n* 2 tablespoons butter, melted\\n* Butter or oil for greasing the pan\\n\\nInstructions:\\n\\n1. In a large bowl, whisk together the flour, sugar, baking powder, and salt.\\n2. In a separate bowl, whisk together the milk, egg, and melted butter.\\n3. Pour the wet ingredients into the dry ingredients and stir until just combined. The batter should still be slightly lumpy.\\n4. Heat a non-stick pan or griddle over medium heat. Grease the pan with butter or oil.\\n5. Using a 1/4 cup measuring cup, scoop the batter onto the pan.\\n6. Cook the pancakes for 2-3 minutes, until bubbles appear on the surface and the edges start to dry.\\n7. Flip the pancakes and cook for another 1-2 minutes, until golden brown.\\n8. Serve the pancakes hot with your favorite toppings, such as maple syrup, butter, fruit, or whipped cream.\\n\\nI hope this helps! Let me know if you have any questions or if you'd like any variations on the recipe.\",\n",
              " 'metadata': [{'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: It's best if we just post random topics here to test the semantic search.\",\n",
              "   'time': '1687166901.338569',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.5207589546579787},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Just for test case we should add some picture to see how slack handles it',\n",
              "   'time': '1687177638.199629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.5110480835981679},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: And we should also post some stickers... :grinning: :smile: :grin:',\n",
              "   'time': '1687169124.658349',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.5097355684494075},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: <https://haystack.deepset.ai/tutorials/08_preprocessing>',\n",
              "   'time': '1687166202.864639',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.5056891748798463},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Bzw I already tries this a while ago but forgot to share this is what the export looks like',\n",
              "   'time': '1687188005.017139',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.4852586414927895},\n",
              "  {'channel': 'random',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: Random shiii',\n",
              "   'time': '1687166237.151359',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.4850804430665586},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is interesting',\n",
              "   'time': '1687177484.530639',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.4773678008202298},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: hello',\n",
              "   'time': '1687166197.580079',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.47375335428880283},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: yeah then we’ll see how we can clean the data',\n",
              "   'time': '1687167171.439409',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.47320763443944247},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Tollan: Also, welcome <@U05D4M7RGQ3>',\n",
              "   'time': '1687169174.199539',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.4717307376767701},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: Sample audio as well',\n",
              "   'time': '1687178138.528629',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.46416148079998976},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) Eyob: This is a good book for design nerds like me',\n",
              "   'time': '1687177700.175079',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.4598987989450203},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': '(2023-06-19) kenenisaalemayhu0: but we can’t use it for search since it’ll be very small',\n",
              "   'time': '1687167197.595189',\n",
              "   'user_id': 'U05DHDPL4FK',\n",
              "   'user_name': 'kenenisaalemayhu0',\n",
              "   'score': 0.4582838512905363},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Good work.. now we don't have to worry about exporting data.\",\n",
              "   'time': '1687166814.786429',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.4538820718560115},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Tollan: Oh.. u r right.. the search probably won't work properly with a small amount of data.\",\n",
              "   'time': '1687168952.621439',\n",
              "   'user_id': 'U05CQ93C3FZ',\n",
              "   'user_name': 'Tollan',\n",
              "   'score': 0.45190041698841477},\n",
              "  {'channel': 'general',\n",
              "   'date': '2023-06-19',\n",
              "   'message': \"(2023-06-19) Eyob: I didn't know slack had a dark mode until I saw the screenshot\",\n",
              "   'time': '1687177520.385329',\n",
              "   'user_id': 'U05D1SQDNSH',\n",
              "   'user_name': 'Eyob aschenaki',\n",
              "   'score': 0.45059140652248275}]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Creating an API Endpoint"
      ],
      "metadata": {
        "id": "pB-U-Yxjp4Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "\n",
        "port_no = 5000\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "ngrok_auth_token = os.environ[\"NGROK_AUTH_TOKEN\"]\n",
        "\n",
        "ngrok.set_auth_token( ngrok_auth_token )\n",
        "public_url =  ngrok.connect(port_no).public_url\n",
        "\n",
        "@app.route(\"/\", methods=['GET', 'POST'])\n",
        "def semantic_search_query():\n",
        "\n",
        "  if request.method == 'GET':\n",
        "    query = request.args.get('query')\n",
        "    return semantic_search(query)\n",
        "\n",
        "  elif request.method == 'POST':\n",
        "    query = request.json['query']\n",
        "    return semantic_search(query)\n",
        "\n",
        "\n",
        "print(f\"Public url for the API... {public_url}\")\n",
        "\n",
        "app.run(port=port_no)"
      ],
      "metadata": {
        "id": "2DeAXHDCkz5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d1f3573-3741-4f64-99e1-62c806520ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-08-23T08:58:04+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public url for the API... https://c4ec-34-16-171-179.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding query ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n",
            "INFO:werkzeug:127.0.0.1 - - [23/Aug/2023 08:59:27] \"POST / HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmlOIW8yAPJo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}